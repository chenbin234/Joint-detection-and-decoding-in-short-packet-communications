{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file is to train the model with the CNN AutoEncoder architecture\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 1. Import Libraries\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"src/\")\n",
    "import os\n",
    "from train.train_utils import training_loop\n",
    "from models.CNN_AutoEncoder import CNN_AutoEncoder\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CNN_AutoEncoder(\n",
    "        M1=config.M1,\n",
    "        M2=config.M2,\n",
    "        N_prime=config.N_prime,\n",
    "        k=config.k,\n",
    "        L=config.L,\n",
    "        n=config.n,\n",
    "        k_mod=config.k_mod,\n",
    "    )\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    return model, loss_fn, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(\n",
    "        project=\"Joint-detection-and-decoding-in-short-packet-communications\",\n",
    "        config=hyperparameters,\n",
    "    ):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, loss_fn, optimizer = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        model = training_loop(\n",
    "            model_type=config.model_type,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,\n",
    "            num_epochs=config.epochs,\n",
    "            training_steps=config.training_steps,\n",
    "            batch_size=config.batch_size,\n",
    "            start_epoch=1,\n",
    "            print_every=None,\n",
    "            save_model_name=config.save_model_name,\n",
    "            save_every=10,\n",
    "            snr_min=config.snr_min,\n",
    "            snr_max=config.snr_max,\n",
    "            k=config.k,\n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "    torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "    torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "    # Initialize Weights and Biases\n",
    "    wandb.login()\n",
    "\n",
    "    time = datetime.now().strftime(\"%Y%m%d_%H_%M_%S\")\n",
    "    # defining model save location\n",
    "    save_model_name = \"CNN_AutoEncoder_\" + time\n",
    "    # create the folder if it doesn't exist\n",
    "    if not os.path.exists(f\"./models/{save_model_name}\"):\n",
    "        os.makedirs(f\"./models/{save_model_name}\")\n",
    "\n",
    "    config = dict(\n",
    "        model_type=\"CNN_AutoEncoder\",\n",
    "        trainable_parameters=0,\n",
    "        train_dataset_path=\"data/processed/train_dataset_info_bits_2000_1_64.pt\",\n",
    "        val_dataset_path=\"data/processed/val_dataset_info_bits_2000_1_64.pt\",\n",
    "        epochs=2,\n",
    "        training_steps=3,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-3,\n",
    "        M1=200,\n",
    "        M2=100,\n",
    "        N_prime=4,\n",
    "        k=64,\n",
    "        L=64,\n",
    "        n=128,\n",
    "        k_mod=2,\n",
    "        snr_min=1,\n",
    "        snr_max=5,\n",
    "        save_model_name=save_model_name,\n",
    "    )\n",
    "\n",
    "    # Build, train and analyze the model with the pipeline\n",
    "    model = model_pipeline(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
